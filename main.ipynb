{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------- [START 2019-01-14 16:16:26] ---------------------------------------------------\n",
      "\n",
      "                           |------------ Train -------------|----------- Valid -------------|----------Best Results---------|------------|\n",
      "mode     iter     epoch    |         loss   f1_macro        |         loss   f1_macro       |         loss   f1_macro       | time       |\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "class weight = tensor([0.6997, 3.2899, 2.0262, 3.2096, 2.7772, 2.6309, 3.0972, 2.1710, 5.9401,\n",
      "        6.0368, 6.1160, 3.6265, 3.6089, 4.0352, 3.4220, 7.1769, 4.1576, 5.2197,\n",
      "        3.7741, 3.1115, 5.2378, 1.7870, 3.4083, 2.0758, 5.2609, 0.7915, 4.7604,\n",
      "        6.4758])\n",
      "finish prepare data\n",
      "train  0.05   0.00         |         0.356  0.159           |         inf  0.0000         |         inf  0    |  0 hr 08 min"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92baaa51fb2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;31m# val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-92baaa51fb2d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, valid_loss, best_results, start)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mnp_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time \n",
    "import json \n",
    "import torch \n",
    "import random \n",
    "import warnings\n",
    "import torchvision\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from utils import *\n",
    "from data import HumanDataset\n",
    "from tqdm import tqdm \n",
    "from config import config\n",
    "from datetime import datetime\n",
    "from models.model import *\n",
    "from torch import nn,optim\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision import transforms as T\n",
    "from imgaug import augmenters as iaa\n",
    "import math\n",
    "# 1. set random seed\n",
    "random.seed(2050)\n",
    "np.random.seed(2050)\n",
    "torch.manual_seed(2050)\n",
    "torch.cuda.manual_seed_all(2050)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not os.path.exists(\"./logs/\"):\n",
    "    os.mkdir(\"./logs/\")\n",
    "\n",
    "log = Logger()\n",
    "log.open(\"logs/%s_log_train.txt\"%config.model_name,mode=\"a\")\n",
    "log.write(\"\\n----------------------------------------------- [START %s] %s\\n\\n\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '-' * 51))\n",
    "log.write('                           |------------ Train -------------|----------- Valid -------------|----------Best Results---------|------------|\\n')\n",
    "log.write('mode     iter     epoch    |         loss   f1_macro        |         loss   f1_macro       |         loss   f1_macro       | time       |\\n')\n",
    "log.write('-------------------------------------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "def train(train_loader,model,criterion,optimizer,epoch,valid_loss,best_results,start):\n",
    "    losses = AverageMeter()\n",
    "    f1 = AverageMeter()\n",
    "    model.train()\n",
    "    y, preds = None, None\n",
    "    for i,(images,target) in enumerate(train_loader):\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output,target)\n",
    "        losses.update(loss.item(),images.size(0))\n",
    "        \n",
    "        np_target = target.cpu().data.numpy()\n",
    "        np_output = output.sigmoid().cpu().data.numpy()\n",
    "        if y is None:\n",
    "            y = np_target\n",
    "            preds = np_output\n",
    "        else:\n",
    "            y = np.concatenate((y, np_target))\n",
    "            preds = np.concatenate((preds, np_output))\n",
    "                \n",
    "        f1_batch = f1_score(target.cpu(),output.sigmoid().cpu() > config.f1_thr,average='macro')\n",
    "        f1.update(f1_batch,images.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('\\r',end='',flush=True)\n",
    "        message = '%s %5.2f %6.2f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                \"train\", i/len(train_loader) + epoch, epoch,\n",
    "                losses.avg, f1.avg, \n",
    "                valid_loss[0], valid_loss[1], \n",
    "                str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                time_to_str((timer() - start),'min'))\n",
    "        print(message , end='',flush=True)\n",
    "    log.write(\"\\n\")\n",
    "    all_f1 = f1_score(y,preds > config.f1_thr,average='macro')\n",
    "    return [losses.avg,all_f1]\n",
    "\n",
    "# 2. evaluate fuunction\n",
    "def evaluate(val_loader,model,criterion,epoch,train_loss,best_results,start):\n",
    "    # only meter loss and f1 score\n",
    "    losses = AverageMeter()\n",
    "    f1 = AverageMeter()\n",
    "    # switch mode for evaluation\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    y, preds = None, None\n",
    "    with torch.no_grad():\n",
    "        for i, (images,target) in enumerate(val_loader):\n",
    "            images_var = images.cuda(non_blocking=True)\n",
    "            target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
    "            output = model(images_var)\n",
    "            loss = criterion(output,target)\n",
    "            losses.update(loss.item(),images_var.size(0))\n",
    "            np_target = target.cpu().data.numpy()\n",
    "            np_output = output.sigmoid().cpu().data.numpy()\n",
    "            if y is None:\n",
    "                y = np_target\n",
    "                preds = np_output\n",
    "            else:\n",
    "                y = np.concatenate((y, np_target))\n",
    "                preds = np.concatenate((preds, np_output))\n",
    "            \n",
    "            f1_batch = f1_score(target.cpu(),output.sigmoid().cpu().data.numpy() > config.f1_thr,average='macro')\n",
    "            f1.update(f1_batch,images_var.size(0))\n",
    "            print('\\r',end='',flush=True)\n",
    "            message = '%s   %5.2f %6.2f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                    \"val\", i/len(val_loader) + epoch, epoch,                    \n",
    "                    train_loss[0], train_loss[1], \n",
    "                    losses.avg, f1.avg,\n",
    "                    str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                    time_to_str((timer() - start),'min'))\n",
    "\n",
    "            print(message, end='',flush=True)\n",
    "        log.write(\"\\n\")\n",
    "    all_f1 = f1_score(y,preds > config.f1_thr,average='macro')\n",
    "    return [losses.avg,all_f1]\n",
    "\n",
    "# 3. test model on public dataset and save the probability matrix\n",
    "def test(test_files,model,folds, res_thr, data_list, flag):\n",
    "    sample_submission_df = data_list\n",
    "    if flag == \"val\":\n",
    "        sample_submission_df.to_csv('./submit/%s/bestloss_submission_%s_label.csv'%(folds, flag), index=None)\n",
    "    \n",
    "    test_gen = HumanDataset(test_files,config.test_data,augument=False,mode=\"test\")\n",
    "    test_loader = DataLoader(test_gen,1,shuffle=False,pin_memory=True,num_workers=2)\n",
    "    #3.1 confirm the model converted to cuda\n",
    "    filenames,labels ,submissions= [],[],[]\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    submit_results = []\n",
    "    for i,(input,filepath) in enumerate(tqdm(test_loader)):\n",
    "        #3.2 change everything to cuda and get only basename\n",
    "        filepath = [os.path.basename(x) for x in filepath]\n",
    "        input = input.squeeze(0)\n",
    "        with torch.no_grad():\n",
    "            image_var = input.cuda(non_blocking=True)\n",
    "            y_pred = model(image_var)\n",
    "            label = y_pred.sigmoid().cpu().data.numpy()\n",
    "            label = np.mean(label, axis = 0)\n",
    "            label = np.expand_dims(label, axis = 0)\n",
    "            \n",
    "            if flag == \"submit\":\n",
    "                max_label_index, max_label_predict = 0, 0\n",
    "                for i in range(28):\n",
    "                    if label[0, i] > max_label_predict:\n",
    "                        max_label_predict = label[0, i]\n",
    "                        max_label_index = i\n",
    "                    if label[0, i] > res_thr[i]:\n",
    "                        label[0, i] = 1\n",
    "                    else:\n",
    "                        label[0, i] = 0    \n",
    "                label[0, max_label_index] = 1\n",
    "                labels.append(label > config.f1_thr)\n",
    "            else:\n",
    "                labels.append(label)\n",
    "            filenames.append(filepath)\n",
    "    for row in np.concatenate(labels):\n",
    "        if flag == \"submit\":\n",
    "            subrow = ' '.join(list([str(i) for i in np.nonzero(row)[0]]))\n",
    "        else:\n",
    "            subrow = ' '.join(list([str(i) for i in row]))\n",
    "        submissions.append(subrow)\n",
    "    sample_submission_df['Predicted'] = submissions\n",
    "    sample_submission_df.to_csv('./submit/%s/bestloss_submission_%s_pred.csv'%(folds, flag), index=None)\n",
    "\n",
    "def getClassWeight(all_files, mu = 0.5):\n",
    "    \"\"\"\n",
    "    Return class weight in all_files\n",
    "    \"\"\"\n",
    "    res = Variable(torch.zeros(config.num_classes))\n",
    "    targets = all_files['Target']\n",
    "    all_n = len(targets)\n",
    "    for target in targets:\n",
    "        class_set = target.split(' ')\n",
    "        for item in class_set:\n",
    "            res[int(item)] += 1\n",
    "    total = res.sum()\n",
    "    for i in range(config.num_classes):\n",
    "        res[i] = math.log(mu * total / float(res[i]))\n",
    "    #for i in range(28):\n",
    "        #res[1, i] = 0.1 * all_n / (0.1 * all_n + (all_n - res[0, i]))\n",
    "        #res[0, i] = 0.1 * all_n / (0.1 * all_n + res[0, i])\n",
    "        #res[1, i] = (all_n - res[0, i]) / res[0, i]\n",
    "        #res[0, i] = 1\n",
    "    print(\"class weight = {0}\".format(res))\n",
    "    return res\n",
    "\n",
    "def getMultiCsv(files_info):\n",
    "    \"\"\"\n",
    "    Read csv info from official and external\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for file_info in files_info:\n",
    "        files = pd.read_csv(file_info[\"file_name\"])\n",
    "        for i in range(len(files)):\n",
    "            files[\"Id\"][i] = file_info[\"prefix\"] + files[\"Id\"][i] + file_info[\"suffix\"]\n",
    "        res.append(files)\n",
    "    res = pd.concat(res, ignore_index=True)\n",
    "    return res\n",
    "\n",
    "def oversample(file_list):\n",
    "    \"\"\"\n",
    "    Oversample train sample with rare class(8, 9, 10, 15, 16, 17, 20, 24, 26, 27) * 2\n",
    "    \"\"\"\n",
    "    rare_class_index = []\n",
    "    for i in file_list.index:\n",
    "        if file_list.iloc[i].Target in [\"8\",\"9\",\"10\",\"15\",\"16\",\"17\",\"20\",\"24\",\"26\",\"27\"]:\n",
    "            rare_class_index.append(i)\n",
    "    res = pd.concat([file_list, file_list.iloc[rare_class_index]], ignore_index=True)\n",
    "    return res\n",
    "\n",
    "train_file_info = [{\"file_name\": \"../input/train.csv\", \"prefix\": \"train/\", \"suffix\": \".png0\"},\\\n",
    "                    {\"file_name\": \"../input/HPAv18/HPAv18RBGY_wodpl.csv\", \"prefix\": \"HPAv18/png_gray/\", \"suffix\": \".png0\"}]\n",
    "all_files = getMultiCsv(train_file_info)\n",
    "\n",
    "test_files = getMultiCsv([{\"file_name\": \"../input/test.csv\", \"prefix\": \"test/\", \"suffix\": \".png0\"}])\n",
    "\n",
    "# criterion\n",
    "# BCEWithLogitsLoss + class weight\n",
    "class_w = getClassWeight(all_files)\n",
    "criterion = nn.BCEWithLogitsLoss(weight = class_w).cuda()\n",
    "# Focal Loss\n",
    "#class_w_one = Variable(torch.ones(config.num_classes))\n",
    "#criterion = MyFocalLoss(weight = class_w_one, gamma = 2).cuda()\n",
    "        \n",
    "for fold_k in range(0, config.kfoldN):\n",
    "    fold = \"model_1_8_%s\"%str(fold_k)\n",
    "    # 4.1 mkdirs\n",
    "    if not os.path.exists(config.submit):\n",
    "        os.makedirs(config.submit)\n",
    "    if not os.path.exists(config.weights + config.model_name + os.sep +str(fold)):\n",
    "        os.makedirs(config.weights + config.model_name + os.sep +str(fold))\n",
    "    if not os.path.exists(config.best_models):\n",
    "        os.mkdir(config.best_models)\n",
    "    if not os.path.exists(config.submit + os.sep + fold):\n",
    "        os.mkdir(config.submit + os.sep + fold)\n",
    "    if not os.path.exists(\"./logs/\"):\n",
    "        os.mkdir(\"./logs/\")\n",
    "    \n",
    "    # 4.2 get model\n",
    "    model = get_net_channel3()\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(),lr = config.lr,momentum=0.9,weight_decay=1e-4)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_loss = 999\n",
    "    best_f1 = 0\n",
    "    best_results = [np.inf,0]\n",
    "    val_metrics = [np.inf,0]\n",
    "    resume = False\n",
    "\n",
    "    train_data_list = pd.read_csv('./Train_Val_MultiLabel_KFold/Train_KFold_%s_%s.csv'%(config.kfoldN, str(fold_k)))\n",
    "    train_data_list = oversample(train_data_list)\n",
    "    val_data_list = pd.read_csv('./Train_Val_MultiLabel_KFold/Val_KFold_%s_%s.csv'%(config.kfoldN, str(fold_k)))\n",
    "    test_data_list = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "    print(\"finish prepare data\")\n",
    "\n",
    "    # load dataset\n",
    "    train_gen = HumanDataset(train_data_list,config.train_data,mode=\"train\")\n",
    "    train_loader = DataLoader(train_gen,batch_size=config.batch_size,shuffle=True,pin_memory=True,num_workers=2)\n",
    "\n",
    "    val_gen = HumanDataset(val_data_list,config.train_data,augument=False,mode=\"train\")\n",
    "    val_loader = DataLoader(val_gen,batch_size=config.batch_size,shuffle=False,pin_memory=True,num_workers=2)\n",
    "\n",
    "    test_gen = HumanDataset(test_files,config.test_data,augument=False,mode=\"test\")\n",
    "    test_loader = DataLoader(test_gen,1,shuffle=False,pin_memory=True,num_workers=2)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(optimizer,step_size=2,gamma=0.1)\n",
    "    start = timer()\n",
    "    \n",
    "    #train\n",
    "    for epoch in range(start_epoch, config.epochs):\n",
    "        if epoch in [2]:\n",
    "            continue\n",
    "        scheduler.step(epoch)\n",
    "        # train\n",
    "        lr = get_learning_rate(optimizer)\n",
    "        train_metrics = train(train_loader,model,criterion,optimizer,epoch,val_metrics,best_results,start)\n",
    "        # val\n",
    "        val_metrics = evaluate(val_loader,model,criterion,epoch,train_metrics,best_results,start)\n",
    "        # check results \n",
    "        is_best_loss = val_metrics[0] < best_results[0]\n",
    "        best_results[0] = min(val_metrics[0],best_results[0])\n",
    "        is_best_f1 = val_metrics[1] > best_results[1]\n",
    "        best_results[1] = max(val_metrics[1],best_results[1])   \n",
    "        # save model\n",
    "        save_checkpoint({\n",
    "                \"epoch\":epoch + 1,\n",
    "                \"model_name\":config.model_name,\n",
    "                \"state_dict\":model.state_dict(),\n",
    "                \"best_loss\":best_results[0],\n",
    "                \"optimizer\":optimizer.state_dict(),\n",
    "                \"fold\":fold,\n",
    "                \"best_f1\":best_results[1],\n",
    "        },is_best_loss,is_best_f1,fold, epoch)\n",
    "        # print logs\n",
    "        print('\\r',end='',flush=True)\n",
    "        log.write('%s  %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                \"best\", epoch, epoch,                    \n",
    "                train_metrics[0], train_metrics[1], \n",
    "                val_metrics[0], val_metrics[1],\n",
    "                str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                time_to_str((timer() - start),'min'))\n",
    "            )\n",
    "        log.write(\"\\n\")\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    best_model = torch.load(\"%s/%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "    model.load_state_dict(best_model[\"state_dict\"])\n",
    "    \n",
    "    # get classify resualt in test data and save to file\n",
    "    test(test_files,model,fold, [0.2] * 28, test_data_list, \"submit\")\n",
    "    # get prediction value in test data and save to file\n",
    "    test(test_files,model,fold, [0.2] * 28, test_data_list, \"test\")\n",
    "    # get label and prediction value in val data and save to file\n",
    "    test(val_data_list,model,fold, [0.2] * 28, val_data_list, \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
